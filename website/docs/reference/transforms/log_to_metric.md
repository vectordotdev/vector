---
last_modified_on: "2020-06-10"
component_title: "Log to Metric"
description: "The Vector `log_to_metric` transform accepts `log` events, but outputs [`metric`](#metric) events, allowing you to convert logs into one or more metrics."
event_types: ["log","metric"]
function_category: "convert"
issues_url: https://github.com/timberio/vector/issues?q=is%3Aopen+is%3Aissue+label%3A%22transform%3A+log_to_metric%22
operating_systems: ["Linux","MacOS","Windows"]
sidebar_label: "log_to_metric|[\"log\",\"metric\"]"
source_url: https://github.com/timberio/vector/tree/master/src/transforms/log_to_metric.rs
status: "prod-ready"
title: "Log to Metric Transform"
unsupported_operating_systems: []
---

import Fields from '@site/src/components/Fields';
import Field from '@site/src/components/Field';
import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';

The Vector `log_to_metric` transform
accepts [`log`][docs.data-model.log] events, but outputs
[`metric`][docs.data-model.metric] events, allowing you to convert logs into
one or more metrics.

<!--
     THIS FILE IS AUTOGENERATED!

     To make changes please edit the template located at:

     website/docs/reference/transforms/log_to_metric.md.erb
-->

## Configuration

```toml title="vector.toml"
[transforms.log_to_metric]
  type = "log_to_metric"

  [[transforms.log_to_metric.metrics]]
    type = "histogram"
    field = "time"
    name = "time_ms" # optional
    tags.status = "{{status}}" # optional
    tags.host = "{{host}}" # optional
    tags.env = "${ENV}" # optional
```

<Fields filters={true}>
<Field
  common={true}
  defaultValue={null}
  enumValues={null}
  examples={[]}
  groups={[]}
  name={"metrics"}
  path={null}
  relevantWhen={null}
  required={true}
  templateable={false}
  type={"[table]"}
  unit={null}
  warnings={[]}
  >

### metrics

A table of key/value pairs representing the keys to be added to the event.


<Fields filters={false}>
<Field
  common={true}
  defaultValue={null}
  enumValues={null}
  examples={["duration","parent.child"]}
  groups={[]}
  name={"field"}
  path={"metrics"}
  relevantWhen={null}
  required={true}
  templateable={false}
  type={"string"}
  unit={null}
  warnings={[]}
  >

#### field

The log field to use as the metric.
 See [Null Fields](#null-fields) for more info.


</Field>
<Field
  common={false}
  defaultValue={false}
  enumValues={null}
  examples={[false,true]}
  groups={[]}
  name={"increment_by_value"}
  path={"metrics"}
  relevantWhen={{"type":"counter"}}
  required={false}
  templateable={false}
  type={"bool"}
  unit={null}
  warnings={[]}
  >

#### increment_by_value

If `true` the metric will be incremented by the [`field`](#field) value. If `false` the
metric will be incremented by 1 regardless of the [`field`](#field) value.



</Field>
<Field
  common={true}
  defaultValue={null}
  enumValues={null}
  examples={["duration_total"]}
  groups={[]}
  name={"name"}
  path={"metrics"}
  relevantWhen={null}
  required={true}
  templateable={false}
  type={"string"}
  unit={null}
  warnings={[]}
  >

#### name

The name of the metric. Defaults to `<field>_total` for `counter` and `<field>`
for `gauge`.



</Field>
<Field
  common={true}
  defaultValue={null}
  enumValues={null}
  examples={[]}
  groups={[]}
  name={"tags"}
  path={"metrics"}
  relevantWhen={null}
  required={false}
  templateable={false}
  type={"table"}
  unit={null}
  warnings={[]}
  >

#### tags

Key/value pairs representing [metric tags][docs.data-model.metric#tags].


<Fields filters={false}>
<Field
  common={true}
  defaultValue={null}
  enumValues={null}
  examples={[{"host":"${HOSTNAME}"},{"region":"us-east-1"},{"status":"{{status}}"}]}
  groups={[]}
  name={"`[tag-name]`"}
  path={"metrics.tags"}
  relevantWhen={null}
  required={true}
  templateable={false}
  type={"string"}
  unit={null}
  warnings={[]}
  >

##### `[tag-name]`

Key/value pairs representing [metric tags][docs.data-model.metric#tags].
Environment variables and field interpolation is allowed.



</Field>
</Fields>

</Field>
<Field
  common={true}
  defaultValue={null}
  enumValues={{"counter":"A [counter metric type][docs.data-model.metric#counter].","gauge":"A [gauge metric type][docs.data-model.metric#gauge].","histogram":"A [distribution metric type][docs.data-model.metric#distribution].","set":"A [set metric type][docs.data-model.metric#set]."}}
  examples={["counter","gauge","histogram","set"]}
  groups={[]}
  name={"type"}
  path={"metrics"}
  relevantWhen={null}
  required={true}
  templateable={false}
  type={"string"}
  unit={null}
  warnings={[]}
  >

#### type

The metric type.



</Field>
</Fields>

</Field>
</Fields>

## Examples

<Tabs
  block={true}
  defaultValue="histograms"
  select={false}
  values={[{"label":"Histograms","value":"histograms"},{"label":"Counts","value":"counts"},{"label":"Sums","value":"sums"},{"label":"Gauges","value":"gauges"},{"label":"Set","value":"set"}]}>

<TabItem value="histograms">

This example demonstrates capturing timings in your logs.

```json title="log event"
{
  "host": "10.22.11.222",
  "message": "Sent 200 in 54.2ms",
  "status": 200,
  "time": 54.2,
}
```

You can convert the `time` field into a `distribution` metric:

```toml tite="vector.toml"
[transforms.log_to_metric]
  type = "log_to_metric"

  [[transforms.log_to_metric.metrics]]
    type = "histogram"
    field = "time"
    name = "time_ms" # optional
    tags.status = "{{status}}" # optional
    tags.host = "{{host}}" # optional
```

A [`metric` event][docs.data-model.metric] will be output with the following
structure:

```javascript title="metric event"
{
  "name": "time_ms",
  "kind": "incremental",
  "tags": {
    "status": "200",
    "host": "10.22.11.222"
  }
  "distribution": {
    "values": [54.2],
    "sample_rates": [1.0]
  }
}
```

This metric will then proceed down the pipeline, and depending on the sink,
will be aggregated in Vector (such is the case for the [`prometheus` sink][docs.sinks.prometheus]) or will be aggregated in the store itself.

</TabItem>

<TabItem value="counts">

This example demonstrates counting HTTP status codes.

Given the following log line:

```json title="log event"
{
  "host": "10.22.11.222",
  "message": "Sent 200 in 54.2ms",
  "status": 200
}
```

You can count the number of responses by status code:

```toml title="vector.toml"
[transforms.log_to_metric]
  type = "log_to_metric"

  [[transforms.log_to_metric.metrics]]
    type = "counter"
    field = "status"
    name = "response_total" # optional
    tags.status = "{{status}}"
    tags.host = "{{host}}"
```

A [`metric` event][docs.data-model.metric] will be output with the following
structure:

```javascript title="metric event"
{
  "name": "response_total",
  "kind": "incremental",
  "tags": {
    "status": "200",
    "host": "10.22.11.222"
  }
  "counter": {
    "value": 1.0,
  }
}
```

This metric will then proceed down the pipeline, and depending on the sink,
will be aggregated in Vector (such is the case for the [`prometheus` sink][docs.sinks.prometheus]) or will be aggregated in the store itself.

</TabItem>

<TabItem value="sums">

In this example we'll demonstrate computing a sum. The scenario we've chosen
is to compute the total of orders placed.

Given the following log line:

```json title="log event"
{
  "host": "10.22.11.222",
  "message": "Order placed for $122.20",
  "total": 122.2
}
```

You can reduce this log into a `counter` metric that increases by the
field's value:

```toml title="vector.toml"
[transforms.log_to_metric]
  type = "log_to_metric"

  [[transforms.log_to_metric.metrics]]
    type = "counter"
    field = "total"
    name = "order_total" # optional
    increment_by_value = true # optional
    tags.host = "{{host}}" # optional
```

A [`metric` event][docs.data-model.metric] will be output with the following
structure:

```javascript title="metric event"
{
  "name": "order_total",
  "kind": "incremental",
  "tags": {
    "status": "200",
    "host": "10.22.11.222"
  }
  "counter": {
    "value": 122.20,
  }
}
```

This metric will then proceed down the pipeline, and depending on the sink,
will be aggregated in Vector (such is the case for the [`prometheus` sink][docs.sinks.prometheus]) or will be aggregated in the store itself.

</TabItem>

<TabItem value="gauges">

In this example we'll demonstrate creating a gauge that represents the current
CPU load verages.

Given the following log line:

```json title="log event"
{
  "host": "10.22.11.222",
  "message": "CPU activity sample",
  "1m_load_avg": 78.2,
  "5m_load_avg": 56.2,
  "15m_load_avg": 48.7
}
```

You can reduce this logs into multiple `gauge` metrics:

```toml title="vector.toml"
[transforms.log_to_metric]
  type = "log_to_metric"

  [[transforms.log_to_metric.metrics]]
    type = "gauge"
    field = "1m_load_avg"
    tags.host = "{{host}}" # optional

  [[transforms.log_to_metric.metrics]]
    type = "gauge"
    field = "5m_load_avg"
    tags.host = "{{host}}" # optional

  [[transforms.log_to_metric.metrics]]
    type = "gauge"
    field = "15m_load_avg"
    tags.host = "{{host}}" # optional
```

Multiple [`metric` events][docs.data-model.metric] will be output with the following
structure:

```javascript title="Metric event 1"
{
  "name": "1m_load_avg",
  "kind": "absolute",
  "tags": {
    "host": "10.22.11.222"
  },
  "gauge": {
    "value": 78.2
  }
}
```

```javascript title="Metric event 2"
{
  "name": "5m_load_avg",
  "kind": "absolute",
  "tags": {
    "host": "10.22.11.222"
  },
  "gauge": {
    "value": 56.2
  }
}
```

```javascript title="Metric event 3"
{
  "name": "15m_load_avg",
  "kind": "absolute",
  "tags": {
    "host": "10.22.11.222"
  },
  "gauge": {
    "value": 48.7
  }
}
```

This metric will then proceed down the pipeline, and depending on the sink,
will be aggregated in Vector (such is the case for the [`prometheus` sink][docs.sinks.prometheus]) or will be aggregated in the store itself.

</TabItem>

<TabItem value="set">

In this example we'll demonstrate how to use sets. Sets are primarly a Statsd
concept that represent the number of unique values seens for a given metric.
The idea is that you pass the unique/high-cardinality value as the metric value
and the metric store will count the number of unique values seen.

For example, given the following log line:

```json title="log event"
{
  "host": "10.22.11.222",
  "message": "Sent 200 in 54.2ms",
  "remote_addr": "233.221.232.22"
}
```

You can count the number of unique `remote_addr` values by using a set:

```toml title="vector.toml"
[transforms.log_to_metric]
  type = "log_to_metric"

  [[transforms.log_to_metric.metrics]]
    type = "set"
    field = "remote_addr"
    tags.host = "{{host}}" # optional
```

A [`metric` event][docs.data-model.metric] will be output with the following
structure:

```javascript title="metric event"
{
  "name": "remote_addr",
  "kind": "incremental",
  "tags": {
    "host": "10.22.11.222"
  },
  "set": {
    "values": ["233.221.232.22"]
  }
}
```

This metric will then proceed down the pipeline, and depending on the sink,
will be aggregated in Vector (such is the case for the [`prometheus` sink][docs.sinks.prometheus]) or will be aggregated in the store itself.

</TabItem>
</Tabs>

## How It Works

### Complex Processing

If you encounter limitations with the `log_to_metric`
transform then we recommend using a [runtime transform][urls.vector_programmable_transforms].
These transforms are designed for complex processing and give you the power of
full programming runtime.

### Environment Variables

Environment variables are supported through all of Vector's configuration.
Simply add `${MY_ENV_VAR}` in your Vector configuration file and the variable
will be replaced before being evaluated.

You can learn more in the
[Environment Variables][docs.configuration#environment-variables] section.

### Multiple Metrics

For clarification, when you convert a single `log` event into multiple [`metric`](#metric)
events, the [`metric`](#metric) events are not emitted as a single array. They are emitted
individually, and the downstream components treat them as individual events.
Downstream components are not aware they were derived from a single log event.

### Null Fields

If the target log [`field`](#field) contains a `null` value it will ignored, and a metric
will not be emitted.

### Reducing

It's important to understand that this transform does not reduce multiple logs
into a single metric. Instead, this transform converts logs into granular
individual metrics that can then be reduced at the edge. Where the reduction
happens depends on your metrics storage. For example, the
[`prometheus` sink][docs.sinks.prometheus] will reduce logs in the sink itself
for the next scrape, while other metrics sinks will proceed to forward the
individual metrics for reduction in the metrics storage itself.


[docs.configuration#environment-variables]: /docs/setup/configuration/#environment-variables
[docs.data-model.log]: /docs/about/data-model/log/
[docs.data-model.metric#counter]: /docs/about/data-model/metric/#counter
[docs.data-model.metric#distribution]: /docs/about/data-model/metric/#distribution
[docs.data-model.metric#gauge]: /docs/about/data-model/metric/#gauge
[docs.data-model.metric#set]: /docs/about/data-model/metric/#set
[docs.data-model.metric#tags]: /docs/about/data-model/metric/#tags
[docs.data-model.metric]: /docs/about/data-model/metric/
[docs.sinks.prometheus]: /docs/reference/sinks/prometheus/
[urls.vector_programmable_transforms]: https://vector.dev/components/?functions%5B%5D=program
