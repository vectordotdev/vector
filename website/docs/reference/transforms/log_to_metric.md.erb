<%- component = metadata.transforms.log_to_metric -%>

<%= component_header(component) %>

<%- if component.requirements.any? -%>
## Requirements

<%= component_requirements(component) %>

<%- end -%>
## Configuration

import CodeHeader from '@site/src/components/CodeHeader';

<CodeHeader text="vector.toml" />

```toml
[transforms.log_to_metric]
  type = "log_to_metric"

  [[transforms.log_to_metric.metrics]]
    type = "histogram"
    field = "time"
    name = "time_ms" # optional
    tags.status = "{{status}}" # optional
    tags.host = "{{host}}" # optional
    tags.env = "${ENV}" # optional
```

<%= fields(component.specific_options_list, heading_depth: 3) %>

<%- if component.env_vars_list.any? -%>
## Env Vars

<%= fields(component.env_vars_list, heading_depth: 3) %>

<%- end -%>
## Output

<Tabs
  block={true}
  defaultValue="timings"
  values={[
    { label: 'Timings', value: 'timings', },
    { label: 'Counting', value: 'counting', },
    { label: 'Summing', value: 'summing', },
    { label: 'Gauges', value: 'gauges', },
    { label: 'Sets', value: 'sets', },
  ]
}>

<TabItem value="timings">

This example demonstrates capturing timings in your logs.

```json
{
  "host": "10.22.11.222",
  "message": "Sent 200 in 54.2ms",
  "status": 200,
  "time": 54.2,
}
```

You can convert the `time` field into a `distribution` metric:

```toml
[transforms.log_to_metric]
  type = "log_to_metric"

  [[transforms.log_to_metric.metrics]]
    type = "histogram"
    field = "time"
    name = "time_ms" # optional
    tags.status = "{{status}}" # optional
    tags.host = "{{host}}" # optional
```

A [`metric` event][docs.data-model.metric] will be output with the following
structure:

```javascript
{
  "name": "time_ms",
  "kind": "incremental",
  "tags": {
    "status": "200",
    "host": "10.22.11.222"
  }
  "value": {
    "type": "distribution",
    "values": [54.2],
    "sample_rates": [1.0]
  }
}
```

This metric will then proceed down the pipeline, and depending on the sink,
will be aggregated in Vector (such is the case for the [`prometheus` \
sink][docs.sinks.prometheus]) or will be aggregated in the store itself.

</TabItem>
<TabItem value="counting">

This example demonstrates counting HTTP status codes.

Given the following log line:

```json
{
  "host": "10.22.11.222",
  "message": "Sent 200 in 54.2ms",
  "status": 200
}
```

You can count the number of responses by status code:

```toml
[transforms.log_to_metric]
  type = "log_to_metric"

  [[transforms.log_to_metric.metrics]]
    type = "counter"
    field = "status"
    name = "response_total" # optional
    tags.status = "{{status}}"
    tags.host = "{{host}}"
```

A [`metric` event][docs.data-model.metric] will be output with the following
structure:

```javascript
{
  "name": "response_total",
  "kind": "incremental",
  "tags": {
    "status": "200",
    "host": "10.22.11.222"
  }
  "value": {
    "type": "counter",
    "value": 1.0,
  }
}
```

This metric will then proceed down the pipeline, and depending on the sink,
will be aggregated in Vector (such is the case for the [`prometheus` \
sink][docs.sinks.prometheus]) or will be aggregated in the store itself.

</TabItem>
<TabItem value="summing">

In this example we'll demonstrate computing a sum. The scenario we've chosen
is to compute the total of orders placed.

Given the following log line:

```json
{
  "host": "10.22.11.222",
  "message": "Order placed for $122.20",
  "total": 122.2
}
```

You can reduce this log into a `counter` metric that increases by the
field's value:

```toml
[transforms.log_to_metric]
  type = "log_to_metric"

  [[transforms.log_to_metric.metrics]]
    type = "counter"
    field = "total"
    name = "order_total" # optional
    increment_by_value = true # optional
    tags.host = "{{host}}" # optional
```

A [`metric` event][docs.data-model.metric] will be output with the following
structure:

```javascript
{
  "name": "order_total",
  "kind": "incremental",
  "tags": {
    "status": "200",
    "host": "10.22.11.222"
  }
  "value": {
    "type": "counter",
    "value": 122.20,
  }
}
```

This metric will then proceed down the pipeline, and depending on the sink,
will be aggregated in Vector (such is the case for the [`prometheus` \
sink][docs.sinks.prometheus]) or will be aggregated in the store itself.

</TabItem>
<TabItem value="gauges">

In this example we'll demonstrate creating a gauge that represents the current
CPU load verages.

Given the following log line:

```json
{
  "host": "10.22.11.222",
  "message": "CPU activity sample",
  "1m_load_avg": 78.2,
  "5m_load_avg": 56.2,
  "15m_load_avg": 48.7
}
```

You can reduce this logs into multiple `gauge` metrics:

```toml
[transforms.log_to_metric]
  type = "log_to_metric"

  [[transforms.log_to_metric.metrics]]
    type = "gauge"
    field = "1m_load_avg"
    tags.host = "{{host}}" # optional

  [[transforms.log_to_metric.metrics]]
    type = "gauge"
    field = "5m_load_avg"
    tags.host = "{{host}}" # optional

  [[transforms.log_to_metric.metrics]]
    type = "gauge"
    field = "15m_load_avg"
    tags.host = "{{host}}" # optional
```

Multiple [`metric` events][docs.data-model.metric] will be output with the following
structure:

```javascript
[
  {
    "name": "1m_load_avg",
    "kind": "absolute",
    "tags": {
      "host": "10.22.11.222"
    },
    "value": {
      "type": "gauge",
      "value": 78.2
    }
  },
  {
    "name": "5m_load_avg",
    "kind": "absolute",
    "tags": {
      "host": "10.22.11.222"
    },
    "value": {
      "type": "gauge",
      "value": 56.2
    }
  },
  {
    "name": "15m_load_avg",
    "kind": "absolute",
    "tags": {
      "host": "10.22.11.222"
    },
    "value": {
      "type": "gauge",
      "value": 48.7
    }
  }
]
```

This metric will then proceed down the pipeline, and depending on the sink,
will be aggregated in Vector (such is the case for the [`prometheus` \
sink][docs.sinks.prometheus]) or will be aggregated in the store itself.

</TabItem>
<TabItem value="sets">

In this example we'll demonstrate how to use sets. Sets are primarly a Statsd
concept that represent the number of unique values seens for a given metric.
The idea is that you pass the unique/high-cardinality value as the metric value
and the metric store will count the number of unique values seen.

For example, given the following log line:

```json
{
  "host": "10.22.11.222",
  "message": "Sent 200 in 54.2ms",
  "remote_addr": "233.221.232.22"
}
```

You can count the number of unique `remote_addr` values by using a set:

```toml
[transforms.log_to_metric]
  type = "log_to_metric"

  [[transforms.log_to_metric.metrics]]
    type = "set"
    field = "remote_addr"
    tags.host = "{{host}}" # optional
```

A [`metric` event][docs.data-model.metric] will be output with the following
structure:

```javascript
{
  "name": "remote_addr",
  "kind": "incremental",
  "tags": {
    "host": "10.22.11.222"
  },
  "value": {
    "type": "set",
    "values": ["233.221.232.22"]
  }
}
```

This metric will then proceed down the pipeline, and depending on the sink,
will be aggregated in Vector (such is the case for the [`prometheus` \
sink][docs.sinks.prometheus]) or will be aggregated in the store itself.

</TabItem>
</Tabs>

## How It Works [[sort]]

<%= component_sections(component) %>

### Multiple Metrics

For clarification, when you convert a single `log` event into multiple `metric`
events, the `metric` events are not emitted as a single array. They are emitted
individually, and the downstream components treat them as individual events.
Downstream components are not aware they were derived from a single log event.

### Null Fields

If the target log `field` contains a `null` value it will ignored, and a metric
will not be emitted.

### Reducing

It's important to understand that this transform does not reduce multiple logs
into a single metric. Instead, this transform converts logs into granular
individual metrics that can then be reduced at the edge. Where the reduction
happens depends on your metrics storage. For example, the
[`prometheus` sink][docs.sinks.prometheus] will reduce logs in the sink itself
for the next scrape, while other metrics sinks will proceed to forward the
individual metrics for reduction in the metrics storage itself.


