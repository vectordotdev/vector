---
last_modified_on: "2020-03-29"
$schema: "/.meta/.schemas/guides.json"
title: "Send logs from a file to Kafka"
description: "A simple guide to send logs from a file to Kafka in just a few minutes."
author_github: https://github.com/binarylogic
cover_label: "File to Kafka Integration"
tags: ["type: tutorial","domain: sources","domain: sinks","source: file","sink: kafka"]
hide_pagination: true
---

import ConfigExample from '@site/src/components/ConfigExample';
import DaemonDiagram from '@site/src/components/DaemonDiagram';
import InstallationCommand from '@site/src/components/InstallationCommand';

Logs are an _essential_ part of observing any
service; without them you are flying blind. But collecting and analyzing them
can be a real challenge -- especially at scale. Not only do you need to solve
the basic task of collecting your logs, but you must do it
in a reliable, performant, and robust manner. Nothing is more frustrating than
having your logs pipeline fall on it's face during an
outage, or even worse, disrupt more important services!

Fear not! In this guide we'll show you how to send send logs from a file to Kafka
and build a logs pipeline that will be the backbone of
your observability strategy.

<!--
     THIS FILE IS AUTOGENERATED!

     To make changes please edit the template located at:

     website/guides/integrate/sources/file/kafka.md.erb
-->

## What We'll Accomplish

To be clear, here's everything we'll accomplish in this short guide:

<ol className="list--checks list--flush">
  <li>
    Tail one or more files.
    <ol>
      <li>Automatically discover new files with glob patterns.</li>
      <li>Merge multi-line logs into one event.</li>
      <li>Checkpoint your position to ensure data is not lost between restarts.</li>
      <li>Enrich your logs with useful file and host-level context.</li>
    </ol>
  </li>
  <li>
    Send logs to Kafka.
    <ol>
      <li>Leverage any of AWS' IAM strategies.</li>
      <li>Optionally compress data to maximize throughput.</li>
      <li>Automatically retry failed requests, with backoff.</li>
      <li>Buffer your data in-memory or on-disk for performance and durability.</li>
    </ol>
  </li>
  <li className="list--li--arrow list--li--pink text--bold">All in just a few minutes!</li>
</ol>

## How It Works

We'll be using [Vector][urls.vector_website] to accomplish this task. Vector
is a [popular][urls.vector_stars], lightweight, and
[ultra-fast][urls.vector_performance] utility for building observability
pipelines. It's written in [Rust][urls.rust], making it memory safe and
reliable. We'll be deploying Vector as a
[daemon][docs.strategies#daemon].

The [daemon deployment strategy][docs.strategies#daemon] is designed for data
collection on a single host. Vector runs in the background, in its own process,
collecting _all_ data for that host.
For this guide, Vector will collect data from
a file via Vector's
[`file`][docs.sources.file].
The following diagram demonstrates how it works.

<DaemonDiagram
  platformName={null}
  sourceName={"file"}
  sinkName={"kafka"} />

## Tutorial

<div className="steps steps--h3">
<ol>
<li>

### Install Vector

<InstallationCommand />

</li>
<li>

### Configure Vector

<ConfigExample
  format="toml"
  path="vector.toml"
  sourceName={"file"}
  sinkName={"kafka"} />

</li>
<li>

### Start Vector

```bash
vector --config vector.toml
```

That's it! Simple and to the point. Hit `ctrl+c` to exit.

</li>
</ol>
</div>


[docs.sources.file]: /docs/reference/sources/file/
[docs.strategies#daemon]: /docs/setup/deployment/strategies/#daemon
[urls.rust]: https://www.rust-lang.org/
[urls.vector_performance]: https://vector.dev/#performance
[urls.vector_stars]: https://github.com/timberio/vector/stargazers
[urls.vector_website]: https://vector.dev
