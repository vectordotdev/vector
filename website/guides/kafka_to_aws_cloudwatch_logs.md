---
id: kafka_to_aws_cloudwatch_logs
title: Writing Kafka Events to AWS Cloudwatch Logs
description: Learn how to send Kafka events to AWS Cloudwatch Logs with optional enrichments.
keywords: ["kafka","aws cloudwatch logs"]
---

In this guide we'll be consuming `kafka` logs and
writing them to a `aws_cloudwatch_logs` sink.

<!--truncate-->

<!--
      THIS FILE IS AUTOGENERATED!

      To make changes please edit the template located at: scripts/generate/templates/guides/guide.md.erb
-->

## Setup

If you haven't already, install Vector. Here's a script for the lazy:

```bash
curl --proto '=https' --tlsv1.2 -sSf https://sh.vector.dev | sh
```

Or [choose your preferred installation method][docs.installation].

## Configure a Source

Next, create a config file in a local directory (called `config.toml`) and add a
`kafka` source by pasting in this snippet:

```toml
[sources.my-source-id]
  # REQUIRED
  type = "kafka" # must be: "kafka"
  bootstrap_servers = "10.14.22.123:9092,10.14.23.332:9092" # example
  group_id = "consumer-group-name" # example
  topics = ["^(prefix1|prefix2)-.+", "topic-1", "topic-2"] # example

  # OPTIONAL
  key_field = "user_id" # example, no default
```

## (Optional) Parse Events

If our logs consumed from Kafka are structured then we should
parse them. We can do that with a range of [transforms][docs.transforms], in
this example we will use the `json_parser` transform:

```toml
[transforms.parser]
  # REQUIRED
  type = "json_parser" # must be: "json_parser"
  inputs = ["my-source-id"] # example
  drop_invalid = true # example

  # OPTIONAL
  drop_field = true # default
  field = "message" # default
```

Note that for the `inputs` field we specify our `kafka` source by
its name `my-source-id`.

This step is optional, if you choose to skip it then remember to set the
`inputs` of the next component to the previous component in the topology
(`my-source-id`).

## (Optional) Enrich Events

We can also choose to enrich our events with [transforms][docs.transforms]. In
this example we're going to add a `aws_ec2_metadata` transform:

```toml
[transforms.enricher]
  # REQUIRED
  type = "aws_ec2_metadata" # must be: "aws_ec2_metadata"
  inputs = ["parser"] # example

  # OPTIONAL
  fields = ["instance-id", "local-hostname", "local-ipv4", "public-hostname", "public-ipv4", "ami-id", "availability-zone", "vpc-id", "subnet-id", "region"] # default
  host = "http://169.254.169.254" # default
  namespace = "" # default
  refresh_interval_secs = 10 # default
```

Note that for the `inputs` field we specify `parser`.

This step is optional, if you choose to skip it then remember to set the
`inputs` of the next component to the previous component in the topology
(`parser`).

## Configure a Sink

Now we configure our sink, making sure to set the input to `enricher`:

```toml
[sinks.my-sink-id]
  # REQUIRED
  type = "aws_cloudwatch_logs" # must be: "aws_cloudwatch_logs"
  inputs = ["enricher"] # example
  group_name = "{{ file }}" # example
  region = "us-east-1" # example
  stream_name = "{{ instance_id }}" # example

  # OPTIONAL
  create_missing_group = true # default
  create_missing_stream = true # default
```

## Run It

That's it, we're ready to execute our pipeline. You can run it locally with:

```sh
vector -c ./config.toml
```

Vector is very flexible and can be deployed in a way that suits your target
environment. For guidance check out our [deployment documentation][docs.deployment].


[docs.deployment]: /docs/setup/deployment/
[docs.installation]: /docs/setup/installation/
[docs.transforms]: /docs/reference/transforms/
