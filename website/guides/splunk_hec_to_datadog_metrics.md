---
id: splunk_hec_to_datadog_metrics
title: Writing Splunk HEC Events to Datadog Metrics
description: Learn how to send Splunk HEC events to Datadog Metrics with optional enrichments.
keywords: ["splunk hec","datadog metrics"]
---

In this guide we'll be consuming `splunk_hec` logs and writing them to a `datadog_metrics` sink.

<!--truncate-->

<!--
      THIS FILE IS AUTOGENERATED!

      To make changes please edit the template located at: scripts/generate/templates/guides/guide.md.erb
-->

## Setup

If you haven't already, install Vector. Here's a script for the lazy:

```bash
curl --proto '=https' --tlsv1.2 -sSf https://sh.vector.dev | sh
```

Or [choose your preferred installation method][docs.installation].

## Configure a Source

Next, create a config file and add a `splunk_hec` source:

```toml
[sources.my-source-id]
  # REQUIRED
  type = "splunk_hec" # must be: "splunk_hec"

  # OPTIONAL
  address = "0.0.0.0:8088" # default
  token = "A94A8FE5CCB19BA61C4C08" # example, no default
```

## (Optional) Parse Events

If our logs consumed from Splunk HEC are structured then we should
parse them. We can do that with a range of [transforms][docs.transforms], in
this example we will use the `json_parser` transform:

```toml
[transforms.parser]
  # REQUIRED
  type = "json_parser" # must be: "json_parser"
  inputs = ["my-source-id"] # example
  drop_invalid = true # example

  # OPTIONAL
  drop_field = true # default
  field = "message" # default
```

Note that for the `inputs` field we specify our `splunk_hec` source by
its name `my-source-id`.

This step is optional, if you choose to skip it then remember to set the
`inputs` of the next component to the previous component in the topology
(`my-source-id`).

## Transform Events

We will need to convert the log events consumed from our
`splunk_hec` source into metrics. We can do that with a
`log_to_metric` transform:

```toml
[transforms.converter]
  # REQUIRED - General
  type = "log_to_metric" # must be: "log_to_metric"
  inputs = ["parser"] # example

  # REQUIRED - Metrics
  [[transforms.converter.metrics]]
    # REQUIRED
    type = "counter" # example, enum
    field = "duration" # example
    name = "duration_total" # example

    # OPTIONAL
    increment_by_value = false # default, relevant when type = "counter"
    [transforms.converter.metrics.tags]
      host = "${HOSTNAME}" # example
      region = "us-east-1" # example
      status = "{{status}}" # example
```

## (Optional) Enrich Events

We can also choose to enrich our events with [transforms][docs.transforms]. In
this example we're going to add a `add_tags` transform:

```toml
[transforms.enricher]
  # REQUIRED - General
  type = "add_tags" # must be: "add_tags"
  inputs = ["converter"] # example

  # REQUIRED - Tags
  [transforms.enricher.tags]
    static_tag = "my value" # example
    env_tag = "${ENV_VAR}" # example
```

Note that for the `inputs` field we specify `converter`.

This step is optional, if you choose to skip it then remember to set the
`inputs` of the next component to the previous component in the topology
(`converter`).

## Configure a Sink

Now we configure our sink, making sure to set the input to `enricher`:

```toml
[sinks.my-sink-id]
  # REQUIRED
  type = "datadog_metrics" # must be: "datadog_metrics"
  inputs = ["enricher"] # example
  api_key = "3111111111111111aaaaaaaaaaaaaaaa" # example
  namespace = "service" # example

  # OPTIONAL
  host = "https://api.datadoghq.com" # default
```

## Run It

That's it, we're ready to execute our pipeline. You can run it locally with:

```sh
vector -c ./config.toml
```

And choose from a range of strategies on [how to deploy it][docs.deployment].


[docs.deployment]: /docs/setup/deployment/
[docs.installation]: /docs/setup/installation/
[docs.transforms]: /docs/reference/transforms/
