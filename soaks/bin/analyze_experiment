#!/usr/bin/env python3
import argparse
import numpy as np
import pandas as pd
import scipy.stats
import common

np.seterr(all='raise')

parser = argparse.ArgumentParser(description='t-test experiments with Welch method')
parser.add_argument('--baseline-sha', type=str, help='the sha of the baseline experiment')
parser.add_argument('--capture-dir', type=str, help='the directory to search for capture files')
parser.add_argument('--comparison-sha', type=str, help='the sha of the comparison experiment')
parser.add_argument('--erratic-soaks', type=str, default='', help='a comma separated list of known-erratic experiments, NOT TO BE USED LIGHTLY')
parser.add_argument('--mean-drift-percentage', type=float, default=8.87, help='the percentage of mean drift we allow in an experiment, expressed as a value from 0 to 100, default 9th percentile')
parser.add_argument('--p-value', type=float, default=0.1, help='the p-value for comparing with t-test results, the smaller the more certain')
parser.add_argument('--vector-cpus', type=int, help='the total number of CPUs given to vector during the experiment')
parser.add_argument('--warmup-seconds', type=int, default=30, help='the number of seconds to treat as warmup')
parser.add_argument('--coefficient-of-variation-limit', type=float, default=0.1, help='the acceptable limit +/- for the ratio of stdev to mean, default 0.1')
parser.add_argument('--report-erratic', type=bool, default=False, help='report on changes in erratic behavior')
args = parser.parse_args()

known_erratic_soaks = args.erratic_soaks.split(',')

bytes_written = pd.concat(common.compute_throughput(
    common.open_captures(args.capture_dir,
                         'bytes_written',
                         unwanted_labels=['metric_name', 'metric_kind', 'target']),
    cpus = args.vector_cpus))
# Skip past warmup seconds samples, allowing for vector warmup to not factor
# into judgement.
bytes_written = bytes_written[(bytes_written.fetch_index > args.warmup_seconds) &
                              (bytes_written.throughput > 0.0)]

results = []
for exp in bytes_written.experiment.unique():
    baseline = bytes_written.loc[(bytes_written.experiment == exp) & (bytes_written.variant == 'baseline')]
    comparison = bytes_written.loc[(bytes_written.experiment == exp) & (bytes_written.variant == 'comparison')]

    baseline_mean = baseline.throughput.mean()
    baseline_stdev = baseline.throughput.std()
    baseline_stderr = scipy.stats.sem(baseline.throughput)
    comparison_mean = comparison.throughput.mean()
    comparison_stdev = comparison.throughput.std()
    comparison_stderr = scipy.stats.sem(comparison.throughput)
    diff =  comparison_mean - baseline_mean
    percent_change = round(((comparison_mean - baseline_mean) / baseline_mean) * 100, 2)

    baseline_outliers = common.total_outliers(baseline)
    comparison_outliers = common.total_outliers(comparison)

    baseline_cov = scipy.stats.variation(baseline.throughput)
    comparison_cov = scipy.stats.variation(comparison.throughput)
    erratic = ((baseline_cov > args.coefficient_of_variation_limit) or
               (comparison_cov > args.coefficient_of_variation_limit))

    # The t-test here is calculating whether the expected mean of our two
    # distributions is equal, or, put another way, whether the samples we have
    # here are from identical distributions. The higher the returned p-value by
    # ttest_ind the more likely it is that the samples _do_ have the same
    # expected mean.
    #
    # If the p-value is below our threshold then it is _unlikely_ that the two
    # samples actually have the same mean -- are from the same distribution --
    # and so there's some statistically interesting difference between the two
    # samples. For our purposes here that implies that performance has changed.
    res = scipy.stats.ttest_ind_from_stats(baseline_mean,
                                           baseline_stdev,
                                           len(baseline),
                                           comparison_mean,
                                           comparison_stdev,
                                           len(comparison),
                                           equal_var=False)
    results.append({'experiment': exp,
                    'Δ mean': diff.mean(),
                    'Δ mean %': percent_change,
                    'baseline mean': baseline_mean,
                    'baseline stdev': baseline_stdev,
                    'baseline stderr': baseline_stderr,
                    'baseline outlier %': (baseline_outliers / len(baseline)) * 100,
                    'baseline CoV': scipy.stats.variation(baseline.throughput),
                    'comparison mean': comparison_mean,
                    'comparison stdev': comparison_stdev,
                    'comparison stderr': comparison_stderr,
                    'comparison outlier %': (comparison_outliers / len(comparison)) * 100,
                    'comparison CoV': scipy.stats.variation(comparison.throughput),
                    'p-value': res.pvalue,
                    'erratic': erratic,
                    'declared erratic': exp in known_erratic_soaks
                    })

results = pd.DataFrame.from_records(results)

print(f'''
# Soak Test Results
Baseline: {args.baseline_sha}
Comparison: {args.comparison_sha}
Total Vector CPUs: {args.vector_cpus}

<details>
<summary>Explanation</summary>
<p>
A soak test is an integrated performance test for vector in a repeatable rig,
with varying configuration for vector.  What follows is a statistical summary of
a brief vector run for each configuration across SHAs given above.  The goal of
these tests are to determine, quickly, if vector performance is changed and to
what degree by a pull request. Where appropriate units are scaled per-core.
</p>

<p>
The table below, if present, lists those experiments that have experienced a
statistically significant change in their throughput performance between
baseline and comparision SHAs, with {(1.0 - args.p_value) * 100}% confidence OR
have been detected as newly erratic. Negative values mean that baseline is
faster, positive comparison. Results that do not exhibit more than a
±{args.mean_drift_percentage}% change in mean throughput are discarded. An
experiment is erratic if its coefficient of variation is greater
than {args.coefficient_of_variation_limit}. The abbreviated table will be
omitted if no interesting changes are observed.
</p>
</details>
''')

drift_filter = results['Δ mean %'].abs() > args.mean_drift_percentage
declared_erratic = results.experiment.isin(known_erratic_soaks)
p_value_violation = results['p-value'] < args.p_value
erratic_violation = results.erratic == True

changes = results[p_value_violation & drift_filter & ~declared_erratic].copy(deep=True)
changes['Δ confidence'] = changes['p-value'].apply(common.confidence)
changes = changes.drop(labels=['p-value', 'baseline mean',
                               'baseline stdev', 'comparison mean',
                               'baseline outlier %', 'baseline CoV',
                               'comparison outlier %', 'comparison CoV', 'erratic',
                               'comparison stdev', 'declared erratic'], axis=1)
changes = changes.sort_values('Δ mean %', ascending=False)
changes['Δ mean'] = changes['Δ mean'].apply(common.human_bytes)
changes['baseline stderr'] = changes['baseline stderr'].apply(common.human_bytes)
changes['comparison stderr'] = changes['comparison stderr'].apply(common.human_bytes)

no_longer_erratic = results[~erratic_violation & declared_erratic].copy(deep=True)
no_longer_erratic = no_longer_erratic.drop(labels=['p-value', 'baseline mean',
                                                   'baseline stdev', 'comparison mean', 'Δ mean',
                                                   'baseline outlier %', 'comparison outlier %',
                                                   'comparison stdev', 'declared erratic'], axis=1)
no_longer_erratic['baseline stderr'] = no_longer_erratic['baseline stderr'].apply(common.human_bytes)
no_longer_erratic['comparison stderr'] = no_longer_erratic['comparison stderr'].apply(common.human_bytes)

actually_erratic = results[erratic_violation & ~declared_erratic].copy(deep=True)
actually_erratic = actually_erratic.drop(labels=['p-value', 'baseline mean',
                                                 'baseline stdev', 'comparison mean', 'Δ mean',
                                                 'baseline outlier %', 'comparison outlier %',
                                                 'comparison stdev', 'declared erratic'], axis=1)
actually_erratic['baseline stderr'] = actually_erratic['baseline stderr'].apply(common.human_bytes)
actually_erratic['comparison stderr'] = actually_erratic['comparison stderr'].apply(common.human_bytes)

detected_changes = len(changes) > 0
detected_not_erratic = len(no_longer_erratic) > 0
detected_actually_erratic = len(actually_erratic) > 0

if detected_changes:
    print(f"Changes in throughput with confidence ≥ {common.confidence(args.p_value)}:")
    print()
    print(changes.to_markdown(index=False, tablefmt='github'))
    print()
else:
    print(f"No interesting changes with confidence ≥ {common.confidence(args.p_value)}:")
    print()

if args.report_erratic and detected_not_erratic:
    print(f"Experiments that were declared erratic but were detected as no longer being so, cutoff {args.coefficient_of_variation_limit}:")
    print()
    print(no_longer_erratic.to_markdown(index=False, tablefmt='github'))
    print()

if args.report_erratic and detected_actually_erratic:
    print(f"Experiments that were not declared erratic but were detected as being so, cutoff {args.coefficient_of_variation_limit}:")
    print()
    print(actually_erratic.to_markdown(index=False, tablefmt='github'))
    print()

print()
print("<details>")
print("<summary>Fine details of change detection per experiment.</summary>")
print()
results = results.sort_values('Δ mean %', ascending=False)
results['Δ mean'] = results['Δ mean'].apply(common.human_bytes)
results['baseline mean'] = results['baseline mean'].apply(common.human_bytes)
results['baseline stdev'] = results['baseline stdev'].apply(common.human_bytes)
results['baseline stderr'] = results['baseline stderr'].apply(common.human_bytes)
results['comparison mean'] = results['comparison mean'].apply(common.human_bytes)
results['comparison stdev'] = results['comparison stdev'].apply(common.human_bytes)
results['comparison stderr'] = results['comparison stderr'].apply(common.human_bytes)
print(results.to_markdown(index=False, tablefmt='github'))
print("</details>")

print("<details>")
print("<summary>Fine details of each soak run.</summary>")
print()
describe = bytes_written.groupby(['experiment', 'variant', 'run_id']).throughput.describe(percentiles=[0.90, 0.95, 0.99])
describe = describe.rename(columns={'50%': 'average', '95%': 'p95', '90%': 'p90', '99%': 'p99'})
describe = describe.sort_values('mean', ascending=False)
describe['skewness'] = bytes_written.groupby(['experiment', 'variant', 'run_id']).throughput.skew()
describe['mean'] = describe['mean'].apply(common.human_bytes)
describe['std'] = describe['std'].apply(common.human_bytes)
describe['min'] = describe['min'].apply(common.human_bytes)
describe['average'] = describe['average'].apply(common.human_bytes)
describe['p90'] = describe['p90'].apply(common.human_bytes)
describe['p95'] = describe['p95'].apply(common.human_bytes)
describe['p99'] = describe['p99'].apply(common.human_bytes)
describe['max'] = describe['max'].apply(common.human_bytes)
print(describe.to_markdown(index=True,
                           tablefmt='github',
                           headers=['(experiment, variant, run_id)', 'total samples',
                                    'mean', 'std', 'min', 'average',
                                    'p90', 'p95', 'p99', 'max', 'skewness']))
print("</details>")
