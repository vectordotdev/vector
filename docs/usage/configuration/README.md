---
description: Vector configuration
---

<!--
     THIS FILE IS AUTOGENERATED!

     To make changes please edit the template located at:

     scripts/generate/templates/docs/usage/configuration/README.md.erb
-->

# Configuration

![][assets.configure]

This section covers configuring Vector and creating [pipelines][docs.configuration#composition]
like the one shown above. Vector requires only a _single_ [TOML][urls.toml]
configurable file, which you can specify via the
[`--config` flag][docs.starting#flags] when [starting][docs.starting] vector:

```bash
vector --config /etc/vector/vector.toml
```

## Example

{% code-tabs %}
{% code-tabs-item title="vector.toml" %}
```coffeescript
# Set global options
data_dir = "/var/lib/vector"

# Ingest data by tailing one or more files
[sources.apache_logs]
  type         = "file"
  include      = ["/var/log/apache2/*.log"]    # supports globbing
  ignore_older = 86400                         # 1 day

# Structure and parse the data
[transforms.apache_parser]
  inputs       = ["apache_logs"]
  type         = "regex_parser"                # fast/powerful regex
  regex        = '^(?P<host>[w.]+) - (?P<user>[w]+) (?P<bytes_in>[d]+) [(?P<timestamp>.*)] "(?P<method>[w]+) (?P<path>.*)" (?P<status>[d]+) (?P<bytes_out>[d]+)$'

# Sample the data to save on cost
[transforms.apache_sampler]
  inputs       = ["apache_parser"]
  type         = "sampler"
  hash_field   = "request_id"                  # sample _entire_ requests
  rate         = 50                            # only keep 50%

# Send structured data to a short-term storage
[sinks.es_cluster]
  inputs       = ["apache_sampler"]            # only take sampled data
  type         = "elasticsearch"
  host         = "http://79.12.221.222:9200"   # local or external host
  index        = "vector-%Y-%m-%d"             # daily indices

# Send structured data to a cost-effective long-term storage
[sinks.s3_archives]
  inputs       = ["apache_parser"]             # don't sample for S3
  type         = "aws_s3"
  region       = "us-east-1"
  bucket       = "my-log-archives"
  key_prefix   = "date=%Y-%m-%d"               # daily partitions, hive friendly format
  batch_size   = 10000000                      # 10mb uncompressed
  gzip         = true                          # compress final objects
  encoding     = "ndjson"                      # new line delimited JSON
```
{% endcode-tabs-item %}
{% endcode-tabs %}

## Global Options

### data_dir

`optional` `no default` `type: string` `example: "/var/lib/vector"`

The directory used for persisting Vector state, such as on-disk buffers, file checkpoints, and more. Please make sure the Vector project has write permissions to this dir.

## Sources

| Name  | Description |
|:------|:------------|
| [**`docker`**][docs.sources.docker] | Ingests data through the docker engine daemon and outputs [`log`][docs.data-model.log] events. |
| [**`file`**][docs.sources.file] | Ingests data through one or more local files and outputs [`log`][docs.data-model.log] events. |
| [**`journald`**][docs.sources.journald] | Ingests data through log records from journald and outputs [`log`][docs.data-model.log] events. |
| [**`kafka`**][docs.sources.kafka] | Ingests data through Kafka 0.9 or later and outputs [`log`][docs.data-model.log] events. |
| [**`statsd`**][docs.sources.statsd] | Ingests data through the StatsD UDP protocol and outputs [`metric`][docs.data-model.metric] events. |
| [**`stdin`**][docs.sources.stdin] | Ingests data through standard input (STDIN) and outputs [`log`][docs.data-model.log] events. |
| [**`syslog`**][docs.sources.syslog] | Ingests data through the Syslog 5424 protocol and outputs [`log`][docs.data-model.log] events. |
| [**`tcp`**][docs.sources.tcp] | Ingests data through the TCP protocol and outputs [`log`][docs.data-model.log] events. |
| [**`udp`**][docs.sources.udp] | Ingests data through the UDP protocol and outputs [`log`][docs.data-model.log] events. |
| [**`vector`**][docs.sources.vector] | Ingests data through another upstream Vector instance and outputs [`log`][docs.data-model.log] and [`metric`][docs.data-model.metric] events. |

[+ request a new source][urls.new_source]

## Transforms

| Name  | Description |
|:------|:------------|
| [**`add_fields`**][docs.transforms.add_fields] | Accepts [`log`][docs.data-model.log] events and allows you to add one or more log fields. |
| [**`add_tags`**][docs.transforms.add_tags] | Accepts [`metric`][docs.data-model.metric] events and allows you to add one or more metric tags. |
| [**`coercer`**][docs.transforms.coercer] | Accepts [`log`][docs.data-model.log] events and allows you to coerce log fields into fixed types. |
| [**`field_filter`**][docs.transforms.field_filter] | Accepts [`log`][docs.data-model.log] and [`metric`][docs.data-model.metric] events and allows you to filter events by a log field's value. |
| [**`grok_parser`**][docs.transforms.grok_parser] | Accepts [`log`][docs.data-model.log] events and allows you to parse a log field value with [Grok][urls.grok]. |
| [**`json_parser`**][docs.transforms.json_parser] | Accepts [`log`][docs.data-model.log] events and allows you to parse a log field value as JSON. |
| [**`log_to_metric`**][docs.transforms.log_to_metric] | Accepts [`log`][docs.data-model.log] events and allows you to convert logs into one or more metrics. |
| [**`lua`**][docs.transforms.lua] | Accepts [`log`][docs.data-model.log] events and allows you to transform events with a full embedded [Lua][urls.lua] engine. |
| [**`regex_parser`**][docs.transforms.regex_parser] | Accepts [`log`][docs.data-model.log] events and allows you to parse a log field's value with a [Regular Expression][urls.regex]. |
| [**`remove_fields`**][docs.transforms.remove_fields] | Accepts [`log`][docs.data-model.log] events and allows you to remove one or more log fields. |
| [**`remove_tags`**][docs.transforms.remove_tags] | Accepts [`metric`][docs.data-model.metric] events and allows you to remove one or more metric tags. |
| [**`sampler`**][docs.transforms.sampler] | Accepts [`log`][docs.data-model.log] events and allows you to sample events with a configurable rate. |
| [**`split`**][docs.transforms.split] | Accepts [`log`][docs.data-model.log] events and allows you to split a field's value on a given separator and zip the tokens into ordered field names. |
| [**`tokenizer`**][docs.transforms.tokenizer] | Accepts [`log`][docs.data-model.log] events and allows you to tokenize a field's value by splitting on white space, ignoring special wrapping characters, and zip the tokens into ordered field names. |

[+ request a new transform][urls.new_transform]

## Sinks

| Name  | Description |
|:------|:------------|
| [**`aws_cloudwatch_logs`**][docs.sinks.aws_cloudwatch_logs] | [Batches](#buffers-and-batches) [`log`][docs.data-model.log] events to [AWS CloudWatch Logs][urls.aws_cw_logs] via the [`PutLogEvents` API endpoint](https://docs.aws.amazon.com/AmazonCloudWatchLogs/latest/APIReference/API_PutLogEvents.html). |
| [**`aws_cloudwatch_metrics`**][docs.sinks.aws_cloudwatch_metrics] | [Streams](#streaming) [`metric`][docs.data-model.metric] events to [AWS CloudWatch Metrics][urls.aws_cw_metrics] via the [`PutMetricData` API endpoint](https://docs.aws.amazon.com/AmazonCloudWatch/latest/APIReference/API_PutMetricData.html). |
| [**`aws_kinesis_streams`**][docs.sinks.aws_kinesis_streams] | [Batches](#buffers-and-batches) [`log`][docs.data-model.log] events to [AWS Kinesis Data Stream][urls.aws_kinesis_data_streams] via the [`PutRecords` API endpoint](https://docs.aws.amazon.com/kinesis/latest/APIReference/API_PutRecords.html). |
| [**`aws_s3`**][docs.sinks.aws_s3] | [Batches](#buffers-and-batches) [`log`][docs.data-model.log] events to [AWS S3][urls.aws_s3] via the [`PutObject` API endpoint](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectPUT.html). |
| [**`blackhole`**][docs.sinks.blackhole] | [Streams](#streaming) [`log`][docs.data-model.log] and [`metric`][docs.data-model.metric] events to a blackhole that simply discards data, designed for testing and benchmarking purposes. |
| [**`clickhouse`**][docs.sinks.clickhouse] | [Batches](#buffers-and-batches) [`log`][docs.data-model.log] events to [Clickhouse][urls.clickhouse] via the [`HTTP` Interface][urls.clickhouse_http]. |
| [**`console`**][docs.sinks.console] | [Streams](#streaming) [`log`][docs.data-model.log] and [`metric`][docs.data-model.metric] events to [standard output streams][urls.standard_streams], such as `STDOUT` and `STDERR`. |
| [**`elasticsearch`**][docs.sinks.elasticsearch] | [Batches](#buffers-and-batches) [`log`][docs.data-model.log] events to [Elasticsearch][urls.elasticsearch] via the [`_bulk` API endpoint](https://www.elastic.co/guide/en/elasticsearch/reference/current/docs-bulk.html). |
| [**`file`**][docs.sinks.file] | [Streams](#streaming) [`log`][docs.data-model.log] events to a file. |
| [**`http`**][docs.sinks.http] | [Batches](#buffers-and-batches) [`log`][docs.data-model.log] events to a generic HTTP endpoint. |
| [**`kafka`**][docs.sinks.kafka] | [Streams](#streaming) [`log`][docs.data-model.log] events to [Apache Kafka][urls.kafka] via the [Kafka protocol][urls.kafka_protocol]. |
| [**`prometheus`**][docs.sinks.prometheus] | [Exposes](#exposing-and-scraping) [`metric`][docs.data-model.metric] events to [Prometheus][urls.prometheus] metrics service. |
| [**`splunk_hec`**][docs.sinks.splunk_hec] | [Batches](#buffers-and-batches) [`log`][docs.data-model.log] events to a [Splunk HTTP Event Collector][urls.splunk_hec]. |
| [**`statsd`**][docs.sinks.statsd] | [Streams](#streaming) [`metric`][docs.data-model.metric] events to [StatsD][urls.statsd] metrics service. |
| [**`tcp`**][docs.sinks.tcp] | [Streams](#streaming) [`log`][docs.data-model.log] events to a TCP connection. |
| [**`vector`**][docs.sinks.vector] | [Streams](#streaming) [`log`][docs.data-model.log] events to another downstream Vector instance. |

[+ request a new sink][urls.new_sink]

## How It Works

### Composition

The primary purpose of the configuration file is to compose pipelines. Pipelines
are formed by connecting [sources][docs.sources], [transforms][docs.transforms],
and [sinks][docs.sinks] through the `inputs` option.

Notice in the above example each input references the `id` assigned to a
previous source or transform.

### Data Directory

Vector requires a `data_dir` value for on-disk operations. Currently, the only
operation using this directory are Vector's on-disk buffers. Buffers, by
default, are memory-based, but if you switch them to disk-based you'll need to
specify a `data_dir`.

### Environment Variables

Vector will interpolate environment variables within your configuration file
with the following syntax:

{% code-tabs %}
{% code-tabs-item title="vector.toml" %}
```coffeescript
[transforms.add_host]
    type = "add_fields"
    
    [transforms.add_host.fields]
        host = "${HOSTNAME}"
```
{% endcode-tabs-item %}
{% endcode-tabs %}

The entire `${HOSTNAME}` variable will be replaced, hence the requirement of
quotes around the definition.

#### Escaping

You can escape environment variable by preceding them with a `$` character. For
example `$${HOSTNAME}` will be treated _literally_ in the above environment
variable example.

### Example Location

The location of your Vector configuration file depends on your
[platform][docs.platforms] or [operating system][docs.operating_systems]. For
most Linux based systems the file can be found at `/etc/vector/vector.toml`.

### Format

The Vector configuration file requires the [TOML][urls.toml] format for it's
simplicity, explicitness, and relaxed white-space parsing. For more information,
please refer to the excellent [TOML documentation][urls.toml].

### Template Syntax

Select configuration options support Vector's template syntax to produce
dynamic values derived from the event's data. There are 2 special syntaxes:

1. Strftime specifiers. Ex: `date=%Y/%m/%d`
2. Event fields. Ex: `{{ field_name }}`

Each are described in more detail below.

#### Strftime specifiers

For simplicity, Vector allows you to supply [strftime \
specifiers][urls.strftime_specifiers] directly as part of the value to produce
formatted timestamp values based off of the event's `timestamp` field.

For example, given the following [`log` event][docs.data-model.log]:

```rust
LogEvent {
    "timestamp": chrono::DateTime<2019-05-02T00:23:22Z>,
    "message": "message"
    "host": "my.host.com"
}
```

And the following configuration:

{% code-tabs %}
{% code-tabs-item title="vector.toml" %}
```coffeescript
[sinks.my_s3_sink_id]
  type = "aws_s3"
  key_prefix = "date=%Y-%m-%d"
```
{% endcode-tabs-item %}
{% endcode-tabs %}

Vector would produce the following value for the `key_prefix` field:

```
date=2019-05-02
```

This effectively enables time partitioning.

##### Event fields

In addition to formatting the `timestamp` field, Vector allows you to directly
access event fields with the `{{ <field-name> }}` syntax.

For example, given the following [`log` event][docs.data-model.log]:

```rust
LogEvent {
    "timestamp": chrono::DateTime<2019-05-02T00:23:22Z>,
    "message": "message"
    "application_id":  1
}
```

And the following configuration:

{% code-tabs %}
{% code-tabs-item title="vector.toml" %}
```coffeescript
[sinks.my_s3_sink_id]
  type = "aws_s3"
  key_prefix = "application_id={{ application_id }}/date=%Y-%m-%d"
```
{% endcode-tabs-item %}
{% endcode-tabs %}

Vector would produce the following value for the `key_prefix` field:

```
application_id=1/date=2019-05-02
```

This effectively enables application specific time partitioning.

### Value Types

All TOML values types are supported. For convenience this includes:

* [Strings](https://github.com/toml-lang/toml#string)
* [Integers](https://github.com/toml-lang/toml#integer)
* [Floats](https://github.com/toml-lang/toml#float)
* [Booleans](https://github.com/toml-lang/toml#boolean)
* [Offset Date-Times](https://github.com/toml-lang/toml#offset-date-time)
* [Local Date-Times](https://github.com/toml-lang/toml#local-date-time)
* [Local Dates](https://github.com/toml-lang/toml#local-date)
* [Local Times](https://github.com/toml-lang/toml#local-time)
* [Arrays](https://github.com/toml-lang/toml#array)
* [Tables](https://github.com/toml-lang/toml#table)


[assets.configure]: ../../assets/configure.svg
[docs.configuration#composition]: ../../usage/configuration#composition
[docs.data-model.log]: ../../about/data-model/log.md
[docs.data-model.metric]: ../../about/data-model/metric.md
[docs.operating_systems]: ../../setup/installation/operating-systems
[docs.platforms]: ../../setup/installation/platforms
[docs.sinks.aws_cloudwatch_logs]: ../../usage/configuration/sinks/aws_cloudwatch_logs.md
[docs.sinks.aws_cloudwatch_metrics]: ../../usage/configuration/sinks/aws_cloudwatch_metrics.md
[docs.sinks.aws_kinesis_streams]: ../../usage/configuration/sinks/aws_kinesis_streams.md
[docs.sinks.aws_s3]: ../../usage/configuration/sinks/aws_s3.md
[docs.sinks.blackhole]: ../../usage/configuration/sinks/blackhole.md
[docs.sinks.clickhouse]: ../../usage/configuration/sinks/clickhouse.md
[docs.sinks.console]: ../../usage/configuration/sinks/console.md
[docs.sinks.elasticsearch]: ../../usage/configuration/sinks/elasticsearch.md
[docs.sinks.file]: ../../usage/configuration/sinks/file.md
[docs.sinks.http]: ../../usage/configuration/sinks/http.md
[docs.sinks.kafka]: ../../usage/configuration/sinks/kafka.md
[docs.sinks.prometheus]: ../../usage/configuration/sinks/prometheus.md
[docs.sinks.splunk_hec]: ../../usage/configuration/sinks/splunk_hec.md
[docs.sinks.statsd]: ../../usage/configuration/sinks/statsd.md
[docs.sinks.tcp]: ../../usage/configuration/sinks/tcp.md
[docs.sinks.vector]: ../../usage/configuration/sinks/vector.md
[docs.sinks]: ../../usage/configuration/sinks
[docs.sources.docker]: ../../usage/configuration/sources/docker.md
[docs.sources.file]: ../../usage/configuration/sources/file.md
[docs.sources.journald]: ../../usage/configuration/sources/journald.md
[docs.sources.kafka]: ../../usage/configuration/sources/kafka.md
[docs.sources.statsd]: ../../usage/configuration/sources/statsd.md
[docs.sources.stdin]: ../../usage/configuration/sources/stdin.md
[docs.sources.syslog]: ../../usage/configuration/sources/syslog.md
[docs.sources.tcp]: ../../usage/configuration/sources/tcp.md
[docs.sources.udp]: ../../usage/configuration/sources/udp.md
[docs.sources.vector]: ../../usage/configuration/sources/vector.md
[docs.sources]: ../../usage/configuration/sources
[docs.starting#flags]: ../../usage/administration/starting.md#flags
[docs.starting]: ../../usage/administration/starting.md
[docs.transforms.add_fields]: ../../usage/configuration/transforms/add_fields.md
[docs.transforms.add_tags]: ../../usage/configuration/transforms/add_tags.md
[docs.transforms.coercer]: ../../usage/configuration/transforms/coercer.md
[docs.transforms.field_filter]: ../../usage/configuration/transforms/field_filter.md
[docs.transforms.grok_parser]: ../../usage/configuration/transforms/grok_parser.md
[docs.transforms.json_parser]: ../../usage/configuration/transforms/json_parser.md
[docs.transforms.log_to_metric]: ../../usage/configuration/transforms/log_to_metric.md
[docs.transforms.lua]: ../../usage/configuration/transforms/lua.md
[docs.transforms.regex_parser]: ../../usage/configuration/transforms/regex_parser.md
[docs.transforms.remove_fields]: ../../usage/configuration/transforms/remove_fields.md
[docs.transforms.remove_tags]: ../../usage/configuration/transforms/remove_tags.md
[docs.transforms.sampler]: ../../usage/configuration/transforms/sampler.md
[docs.transforms.split]: ../../usage/configuration/transforms/split.md
[docs.transforms.tokenizer]: ../../usage/configuration/transforms/tokenizer.md
[docs.transforms]: ../../usage/configuration/transforms
[urls.aws_cw_logs]: https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/WhatIsCloudWatchLogs.html
[urls.aws_cw_metrics]: https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/working_with_metrics.html
[urls.aws_kinesis_data_streams]: https://aws.amazon.com/kinesis/data-streams/
[urls.aws_s3]: https://aws.amazon.com/s3/
[urls.clickhouse]: https://clickhouse.yandex/
[urls.clickhouse_http]: https://clickhouse.yandex/docs/en/interfaces/http/
[urls.elasticsearch]: https://www.elastic.co/products/elasticsearch
[urls.grok]: http://grokdebug.herokuapp.com/
[urls.kafka]: https://kafka.apache.org/
[urls.kafka_protocol]: https://kafka.apache.org/protocol
[urls.lua]: https://www.lua.org/
[urls.new_sink]: https://github.com/timberio/vector/issues/new?labels=Type%3A+New+Feature
[urls.new_source]: https://github.com/timberio/vector/issues/new?labels=Type%3A+New+Feature
[urls.new_transform]: https://github.com/timberio/vector/issues/new?labels=Type%3A+New+Feature
[urls.prometheus]: https://prometheus.io/
[urls.regex]: https://en.wikipedia.org/wiki/Regular_expression
[urls.splunk_hec]: http://dev.splunk.com/view/event-collector/SP-CAAAE6M
[urls.standard_streams]: https://en.wikipedia.org/wiki/Standard_streams
[urls.statsd]: https://github.com/statsd/statsd
[urls.strftime_specifiers]: https://docs.rs/chrono/0.3.1/chrono/format/strftime/index.html
[urls.toml]: https://github.com/toml-lang/toml
