---
description: Streams `log` events to Apache Kafka via the Kafka protocol.
---

<!--
     THIS FILE IS AUTOGENERATED!

     To make changes please edit the template located at:

     scripts/generate/templates/docs/usage/configuration/sinks/kafka.md.erb
-->

# kafka sink

![][assets.kafka_sink]


The `kafka` sink [streams](#streaming) [`log`][docs.data-model.log] events to [Apache Kafka][urls.kafka] via the [Kafka protocol][urls.kafka_protocol].

## Config File

{% code-tabs %}
{% code-tabs-item title="vector.toml (simple)" %}
```coffeescript
[sinks.my_sink_id]
  type = "kafka" # must be: "kafka"
  inputs = ["my-source-id"]
  bootstrap_servers = ["10.14.22.123:9092", "10.14.23.332:9092"]
  encoding = "json" # enum: "json" or "text"
  key_field = "user_id"
  topic = "topic-1234"

  # For a complete list of options see the "advanced" tab above.
```
{% endcode-tabs-item %}
{% code-tabs-item title="vector.toml (advanced)" %}
```coffeescript
[sinks.kafka_sink]
  #
  # General
  #

  # The component type
  # 
  # * required
  # * no default
  # * must be: "kafka"
  type = "kafka"

  # A list of upstream source or transform IDs. See Config Composition for more
  # info.
  # 
  # * required
  # * no default
  inputs = ["my-source-id"]

  # A list of host and port pairs that the Kafka client should contact to
  # bootstrap its cluster metadata.
  # 
  # * required
  # * no default
  bootstrap_servers = ["10.14.22.123:9092", "10.14.23.332:9092"]

  # The encoding format used to serialize the events before flushing.
  # 
  # * required
  # * no default
  # * enum: "json" or "text"
  encoding = "json"
  encoding = "text"

  # The log field name to use for the topic key. If unspecified, the key will be
  # randomly generated. If the field does not exist on the log, a blank value
  # will be used.
  # 
  # * required
  # * no default
  key_field = "user_id"

  # The Kafka topic name to write events to.
  # 
  # * required
  # * no default
  topic = "topic-1234"

  # Enables/disables the sink healthcheck upon start.
  # 
  # * optional
  # * default: true
  healthcheck = true

  #
  # Buffer
  #

  [sinks.kafka_sink.buffer]
    # The buffer's type / location. `disk` buffers are persistent and will be
    # retained between restarts.
    # 
    # * optional
    # * default: "memory"
    # * enum: "memory" or "disk"
    type = "memory"
    type = "disk"

    # The behavior when the buffer becomes full.
    # 
    # * optional
    # * default: "block"
    # * enum: "block" or "drop_newest"
    when_full = "block"
    when_full = "drop_newest"

    # The maximum size of the buffer on the disk.
    # 
    # * optional
    # * no default
    # * unit: bytes
    max_size = 104900000

    # The maximum number of events allowed in the buffer.
    # 
    # * optional
    # * default: 500
    # * unit: events
    num_items = 500
```
{% endcode-tabs-item %}
{% endcode-tabs %}

## How It Works

### Delivery Guarantee

This component offers an [**at least once** delivery guarantee][docs.guarantees#at-least-once-delivery]
if your [pipeline is configured to achieve this][docs.guarantees#at-least-once-delivery].

### Encodings

The `kafka` sink encodes events before writing
them downstream. This is controlled via the `encoding` option which accepts
the following options:

| Encoding | Description |
| :------- | :---------- |
| `json` | The payload will be encoded as a single JSON payload. |
| `text` | The payload will be encoded as new line delimited text, each line representing the value of the `"message"` key. |

#### Dynamic encoding

By default, the `encoding` chosen is dynamic based on the explicit/implcit
nature of the event's structure. For example, if this event is parsed (explicit
structuring), Vector will use `json` to encode the structured data. If the event
was not explicitly structured, the `text` encoding will be used.

To further explain why Vector adopts this default, take the simple example of
accepting data over the [`tcp` source][docs.sources.tcp] and then connecting
it directly to the `kafka` sink. It is less
surprising that the outgoing data reflects the incoming data exactly since it
was not explicitly structured.

### Environment Variables

Environment variables are supported through all of Vector's configuration.
Simply add `${MY_ENV_VAR}` in your Vector configuration file and the variable
will be replaced before being evaluated.

You can learn more in the [Environment Variables][docs.configuration#environment-variables]
section.

### Health Checks

Health checks ensure that the downstream service is accessible and ready to
accept data. This check is performed upon sink initialization.

If the health check fails an error will be logged and Vector will proceed to
start. If you'd like to exit immediately upon health check failure, you can
pass the `--require-healthy` flag:

```bash
vector --config /etc/vector/vector.toml --require-healthy
```

And finally, if you'd like to disable health checks entirely for this sink
you can set the `healthcheck` option to `false`.

### Streaming

The `kafka` sink streams data on a real-time
event-by-event basis. It does not batch data.

## Troubleshooting

The best place to start with troubleshooting is to check the
[Vector logs][docs.monitoring#logs]. This is typically located at
`/var/log/vector.log`, then proceed to follow the
[Troubleshooting Guide][docs.troubleshooting].

If the [Troubleshooting Guide][docs.troubleshooting] does not resolve your
issue, please:

1. Check for any [open `kafka_sink` issues][urls.kafka_sink_issues].
2. If encountered a bug, please [file a bug report][urls.new_kafka_sink_bug].
3. If encountered a missing feature, please [file a feature request][urls.new_kafka_sink_enhancement].
4. If you need help, [join our chat/forum community][urls.vector_chat]. You can post a question and search previous questions.

## Resources

* [**Issues**][urls.kafka_sink_issues] - [enhancements][urls.kafka_sink_enhancements] - [bugs][urls.kafka_sink_bugs]
* [**Source code**][urls.kafka_sink_source]


[assets.kafka_sink]: ../../../assets/kafka-sink.svg
[docs.configuration#environment-variables]: ../../../usage/configuration#environment-variables
[docs.data-model.log]: ../../../about/data-model/log.md
[docs.guarantees#at-least-once-delivery]: ../../../about/guarantees.md#at-least-once-delivery
[docs.monitoring#logs]: ../../../usage/administration/monitoring.md#logs
[docs.sources.tcp]: ../../../usage/configuration/sources/tcp.md
[docs.troubleshooting]: ../../../usage/guides/troubleshooting.md
[urls.kafka]: https://kafka.apache.org/
[urls.kafka_protocol]: https://kafka.apache.org/protocol
[urls.kafka_sink_bugs]: https://github.com/timberio/vector/issues?q=is%3Aopen+is%3Aissue+label%3A%22sink%3A+kafka%22+label%3A%22Type%3A+bug%22
[urls.kafka_sink_enhancements]: https://github.com/timberio/vector/issues?q=is%3Aopen+is%3Aissue+label%3A%22sink%3A+kafka%22+label%3A%22Type%3A+enhancement%22
[urls.kafka_sink_issues]: https://github.com/timberio/vector/issues?q=is%3Aopen+is%3Aissue+label%3A%22sink%3A+kafka%22
[urls.kafka_sink_source]: https://github.com/timberio/vector/tree/master/src/sinks/kafka.rs
[urls.new_kafka_sink_bug]: https://github.com/timberio/vector/issues/new?labels=sink%3A+kafka&labels=Type%3A+bug
[urls.new_kafka_sink_enhancement]: https://github.com/timberio/vector/issues/new?labels=sink%3A+kafka&labels=Type%3A+enhancement
[urls.vector_chat]: https://chat.vector.dev
