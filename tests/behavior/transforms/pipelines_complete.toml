[sources.datadog_agent]
type = "datadog_agent"
address = "0.0.0.0:81"

[sources.syslog]
type = "syslog"
mode = "tcp"
address = "0.0.0.0:9000"

[transforms.processing]
type = "pipelines"
inputs = ["datadog_agent", "syslog"]
mode = "linear"

[transforms.processing.logs]
order = [
    "aws_s3_access_logs",
    "sudo"
]

[transforms.processing.logs.pipelines.aws_s3_access_logs]
name = "AWS S3 access logs"
filter.type = "datadog_search"
filter.source = "source:s3"

# Processor 1: Grok Parser: Parsing S3 Access Logs
[[transforms.processing.logs.pipelines.aws_s3_access_logs.transforms]]
type = "remap"
source = '''
. |= parse_grok(
    .message,
    patterns: [
        # Access
        "%{_s3_bucket_owner} %{_s3_bucket} %{_date_access} (?>%{_client_ip}|-) %{_client_id} %{_request_id} %{_s3_operation} %{notSpace} "(?>%{_method} |)%{_url}(?> %{_version}|)" %{_status_code} %{_s3_error_code} (?>%{_bytes_written}|-) (?>%{_object_size}|-) %{_duration} (?>%{_request_processing_time}|-) "%{_referer}" "%{_user_agent}" %{_request_version_id}.*",

        # Fallback
        "%{_s3_bucket_owner} %{_s3_bucket} %{_date_access} (?>%{_client_ip}|-) %{_client_id} %{_request_id} %{_s3_operation}.*"
    ]
    aliases: {
        "_s3_bucket_owner": "%{notSpace:s3.bucket_owner}",
        "_s3_bucket": "%{notSpace:s3.bucket}",
        "_s3_operation": "%{notSpace:s3.operation}",
        "_s3_error_code": "%{notSpace:s3.error_code:nullIf("-")}",
        "_request_processing_time": "%{integer:http.request_processing_time}",
        "_request_id": "%{notSpace:http.request_id}",
        "_request_version_id": "%{notSpace:http.request_version_id:nullIf("-")}",
        "_bytes_written": "%{integer:network.bytes_written}",
        "_bytes_read": "%{integer:network.bytes_read}",
        "_object_size": "%{integer:network.object_size}",
        "_client_ip": "%{ipOrHost:network.client.ip}",
        "_client_id": "%{notSpace:network.client.id}",
        "_version": "HTTP\/%{regex(\"\\d+\\.\\d+\"):http.version}",
        "_url": "%{notSpace:http.url}",
        "_ident": "%{notSpace:http.ident:nullIf("-")}",
        "_user_agent": "%{regex(\"[^\\\\"]*"):http.useragent}",
        "_referer": "%{notSpace:http.referer:nullIf("-")}",
        "_status_code": "%{integer:http.status_code}",
        "_method": "%{word:http.method}",
        "_duration": "%{integer:duration:scale(1000000)}",
        "_date_access \[%{date(\"dd/MMM/yyyy:HH:mm:ss Z\"):date_access}\]"
    }
)
'''

# Processor 2: User-Agent Parser
[[transforms.processing.logs.pipelines.aws_s3_access_logs.transforms]]
type = "remap"
source = '''
.http.useragent_details |= parse_user_agent(.http.useragent)
'''

# Processor 3: Url Parser
[[transforms.processing.logs.pipelines.aws_s3_access_logs.transforms]]
type = "remap"
source = '''
.http.url_details |= parse_user_agent(.http.url)
'''

# Processor 4: Date Remapper: Define date_access as the official timestamp of the log
[[transforms.processing.logs.pipelines.aws_s3_access_logs.transforms]]
type = "remap"
source = '''
schema_set_timestamp(.date_access)
'''

# Processor 5: Category Processor: Categorize status code
[[transforms.processing.logs.pipelines.aws_s3_access_logs.transforms]]
type = "remap"
source = '''
.http.status_category = switch http.status_code {
when 200...299:
    "OK"
when 300...399:
    "notice"
when 400...499:
    "warning"
}
'''

# Processor 6: Status Remapper: Set the log status based on the status code value
[[transforms.processing.logs.pipelines.aws_s3_access_logs.transforms]]
type = "remap"
source = '''
schema_set_status(.http.status_category)
'''

[transforms.processing.logs.pipelines.sudo]
name = "Sudo"
filter.type = "datadog_search"
filter.source = "source:s3"

[[transforms.processing.logs.pipelines.sudo.transforms]]
type = "remap"
source = '''
. |= parse_grok(
    value: .message,
    pattern: "%{_sudo_user} : %{_sudo_tty}( %{_sudo_pwd})? %{_sudo_run_as_user}( %{_sudo_group})?( %{_sudo_tsid})?( %{_sudo_env})? (%{_sudo_command})?; (%{_sudo_lang} )?(%{_sudo_lc_type} )?.*",
    aliases: {
        _sudo_user: "%{notSpace:system.user}",
        _sudo_tty: "TTY=%{notSpace:system.tty} ;",
        _sudo_pwd: "PWD=%{notSpace:system.pwd} ;",
        _sudo_run_as_user: "USER=%{notSpace:system.run_as_user} ;",
        _sudo_group: "GROUP=%{notSpace:system.run_as_group} ;",
        _sudo_tsid: "TSID=%{notSpace:system.tsid} ;",
        _sudo_env: "ENV=%{data:system.env}",
        _sudo_command: "COMMAND=%{data:system.cmd}",
        _sudo_lang: "LANG=%{notSpace:system.lang}",
        _sudo_lc_type: "LC_CTYPE=%{notSpace:system.lc_type}"
    }
)
'''

[transforms.standard_attributes]
type = "remap"
source = '''
.error.stack ?= .exc_info ?? .stack
.http.method ?= .billing_audit_request_method
'''

[transforms.sample]
type = "sample"
inputs = [ "processing" ]
rate = 10

[transforms.log_to_metrics]
type = "log_to_metric"
inputs = [ "processing" ]

[[transforms.log_to_metrics.metrics]]
field = "duration"
name = "duration_total"
namespace = "service"
type = "counter"

[transforms.log_to_metrics.metrics.tags]
host = "${HOSTNAME}"
region = "us-east-1"
status = "{{status}}"

[sinks.aws_s3]
type = "aws_s3"
inputs = ["processing"]
bucket = "my-bucket"
key_prefix = "date=%F/"
compression = "gzip"
region = "us-east-1"
encoding.codec = "text"

[sinks.datadog_logs]
type = "datadog_logs"
inputs = [ "processing" ]
default_api_key = "${DATADOG_API_KEY_ENV_VAR}"
compression = "gzip"

[sinks.datadog_metrics]
type = "datadog_metrics"
inputs = [ "processing" ]
api_key = "${DATADOG_API_KEY_ENV_VAR}"
